% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/functional_ASE.R
\name{fASE}
\alias{fASE}
\title{Perform network embeddings.}
\usage{
fASE(
  adjacency_tensor,
  embedding_dim,
  spline_no,
  spline_order = 4,
  batch_size = NULL,
  kernel_scale = FALSE,
  timestamp_vec = NULL,
  scalable_method = TRUE,
  scalable_dim = 20,
  scalable_power = 6,
  update_method = "Adam",
  adam_parameter = NULL,
  iteration_step = 2500,
  epsilon = 1e-06,
  step_size = 0.1,
  epoch_length = 1,
  parallel = FALSE
)
}
\arguments{
\item{adjacency_tensor}{an n*n*T array where each slice is a snapshot of the dynamic network.}

\item{embedding_dim}{scalar. the dimension of latent functions d.}

\item{spline_no}{scalar. the number of B-spline basis q.}

\item{spline_order}{(optional) scalar. the polynomial order of B-spline basis, default 4}

\item{batch_size}{(optional) scalar. the size of batches in mini-batch algorithm, default max(50,floor(n/20))}

\item{kernel_scale}{(optional) scalar. the scaled parameter in the kernel used for smoothing, default two times the quartile of the time intervals.}

\item{timestamp_vec}{(optional) vector. the observed time points of observation, default 1:T.}

\item{scalable_method}{(optional) logical. whether to use the random-projection based SVD and mini-batch methods, default TRUE.}

\item{scalable_dim}{(optional) scalar. the dimension of random projection if scalable_method = TRUE, default 20.}

\item{scalable_power}{(optional) scalar. the time of power iterations in power method when using  the random-projection based SVD, default 6.}

\item{update_method}{(optional) string. "Adam" if using the Adam algorithm for update; "GD" if using the classical gradient descent algorithm, default "Adam".}

\item{adam_parameter}{(optional) three-element list. a list containing beta1, beta2 and epsilon in the Adam algorithm if update_method = "Adam". default beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8.}

\item{iteration_step}{(optional) scalar. the number of maximal iteration steps for the gradient algorithm, default 2500.}

\item{epsilon}{(optional) scalar. the stopping criterion for the gradient algorithm, default 1e-6.}

\item{step_size}{(optional) scalar. the initial step size of the gradient algorithm, default 0.1.}

\item{epoch_length}{(optional) scalar. iteration numbers in one loop before calculating the objective function, default 1.}

\item{parallel}{(optional) logical. whether to parallel when tuning hyperparameters, defult TRUE.}
}
\value{
a list of five elements containing: n*d fd format functions, n*d*q coefficient tensor, corresponding functional basis object, values of the basis object on timestamps of snapshots
}
\description{
Transform the dynamic network into d-dimensional latent functions for each node via Accelerated fASE.
}
\examples{
ER_generation <- function(n, p) {
  adjacency_matrix <- matrix(0, nrow = n, ncol = n)
  for (i in 2:n) {
    for (j in 1:(i - 1)) {
      adjacency_matrix[i, j] <- sample(c(0, 1), size = 1, prob = c(1 - p, p))
    }
    adjacency_matrix[i, i] <- 0.5
  }
  adjacency_matrix + t(adjacency_matrix)
}
set.seed(10)
n_nodes <- 10
T <- 10
p <- 0.2
dynamic_adjacency <- c()
dynamic_network_adjacency <- array(0, dim = c(n_nodes, n_nodes, T))
for (t in 1:T) {
  adj <- ER_generation(n_nodes, p)
  dynamic_adjacency <- c(dynamic_adjacency, list(list(adj)))
  dynamic_network_adjacency[, , t] <- adj
}
result_ER1 <- fASE(dynamic_network_adjacency, 2, 4, eps = 1, batch_size = NULL)

}
